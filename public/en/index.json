[
{
	"uri": "https://snowplow-incubator.github.io/advanced-analytics-web-accelerator/en/enrich/enrich_1/",
	"title": "Add Enrichments",
	"tags": [],
	"description": "",
	"content": "Snowplow offers a large number of enrichments that can be used to enhance your event data. An enrichment either updates or populates fields of the atomic event or adds a self-describing context to derived_contexts.\nFor this project, we\u0026rsquo;ll enable the IAB, UA parser and YAUAA enrichments in your console:\nThe IAB enrichment requires purchase (included with Snowplow BDP)\nIAB Use the IAB/ABC International Spiders and Bots List to determine whether an event was produced by a user or a robot/spider based on its’ IP address and user agent.\nUA Parser Parse the useragent and attach detailed useragent information to each event.\nYAUAA Parse and analyse the user agent string of an event and extract as many relevant attributes as possible using YAUAA API.\n"
},
{
	"uri": "https://snowplow-incubator.github.io/advanced-analytics-web-accelerator/en/introduction/",
	"title": "Advanced Analytics for Web",
	"tags": [],
	"description": "",
	"content": "\nIntroduction Welcome to the Advanced Analytics for Web accelerator. Once finished, you will be able to build a deeper understanding of customer behavior on your website and use your data to influence business decisions.\nHere you will learn to:\nModel and Visualise Snowplow data using the snowplow-web dbt package and Streamlit using our sample data for Snowflake (no need to have a working pipeline) Set-up Snowplow Tracking and Enrichment Apply what you have learned on your own pipeline to gain insights Who is this guide for? Data practitioners with or without Javascript developer experience who would like to get familiar with Snowplow data. Data practitioners who want to learn how to use the snowplow-web dbt package and set-up tracking using their companies website or single page application, to gain insight from their customers’ behavioural data as quickly as possible..\nWhat you will learn In approximately 2 working days (~13 working hours) you can achieve the following:\nUpload data - Upload a sample Snowplow events dataset to your Snowflake warehouse Model - Configure and run the snowplow-web data model Visualise - Visualise the modelled data with Streamlit Track - Set-up and deploy tracking to your website or single page application Enrich - Add enrichments to your data Next steps - Gain value from your own pipeline data through modeling and visualisation gantt dateFormat HH-mm axisFormat %M section 1. Upload 1h :upload, 00-00, 1m section 2. Model 2h :model, after upload, 2m section 3. Visualise 3h :visualise, after model, 3m section 4. Track 4h :track, after visualise, 4m section 5. Enrich 1h :enrich, after track, 1m section 6. Next steps 2h :next steps, after enrich, 2m Prerequisites Modeling and Visualisation\ndbt CLI installed / dbt Cloud account available New dbt project created and configured Python 3 Installed Snowflake account and a user with access to create schemas and tables Tracking and Enrichment\nSnowplow pipeline Web app to implement tracking Please note that Snowflake will be used for illustration but the snowplow-web dbt package also supports BigQuery, Databricks, Postgres and Redshift. Further adapter support for this accelerator coming soon!\n"
},
{
	"uri": "https://snowplow-incubator.github.io/advanced-analytics-web-accelerator/en/upload/upload_1/",
	"title": "Data upload",
	"tags": [],
	"description": "",
	"content": " Attachments sample_events.csv (35 MB) snowflake_upload.py (9 KB) Python Snowflake Web Interface One option is to load the sample data to the warehouse using Python as described in the below steps. Please download both the sample_events.csv and the snowflake_upload.py files from the attachments at the top of this page as you will need both.\nStep 1: Set up your environment Set up a virtual environment (recommended) and install the snowflake-connector-python package (tested with version 2.7.12).\npython3 -m venv env source env/bin/activate pip install snowflake-connector-python==2.7.12 Step 2: Change variables and connection details Open the snowflake_upload.py file and edit the following before you execute it:\n2.1 Connection details - update username, password and account # Connection details - to be modified, where needed! conn=sf.connect(user=\u0026#39;your_username\u0026#39;,password=\u0026#39;your_password\u0026#39;,account=\u0026#39;your_account\u0026#39;) 2.2 Variables to be modified - warehouse and database # Variables - to be modified, where needed! warehouse=\u0026#39;YOUR_WAREHOUSE\u0026#39; database = \u0026#39;YOUR_DB 2.3 Path to the sample_data.csv # Path to be modified: csv_file = \u0026#39;/Users/your_user/path_to_csv/sample_events.csv\u0026#39; Step 3: Execute snowflake_upload.py It should finish execution within a minute. You should be alerted as soon as each intermediary step finishes:\nSchema created Staging table YOUR_DB.ATOMIC.SAMPLE_EVENTS_STAGED is created Stage dropped, if applicable Stage created File put to stage Data loaded into staging table Target table: YOUR_DB.ATOMIC.SAMPLE_EVENTS is created Staging table: YOUR_DB.ATOMIC.SAMPLE_EVENTS_STAGED is dropped You will now have the ATOMIC.SAMPLE_EVENTS created and loaded with sample data.\nAnother option is to load the sample data to the warehouse using the Snowflake Web Interface as described in the below steps. Please download the sample_events.csv from the attachments. For more details please check out the official Snowflake documentation.\nStep 1: Create the ATOMIC schema If the ATOMIC schema doesn\u0026rsquo;t exist, create it in your target database.\nCREATE SCHEMA IF NOT EXISTS TARGET_DB.ATOMIC Step 2: Create the SAMPLE_EVENTS_BASE table This is where you will load the sample data to.\nCREATE OR REPLACE TABLE TARGET_DB.ATOMIC.SAMPLE_EVENTS_BASE ( APP_ID VARCHAR(255), PLATFORM VARCHAR(255), ETL_TSTAMP TIMESTAMP_NTZ(9), COLLECTOR_TSTAMP TIMESTAMP_NTZ(9) NOT NULL, DVCE_CREATED_TSTAMP TIMESTAMP_NTZ(9), EVENT VARCHAR(128), EVENT_ID VARCHAR(36) NOT NULL, TXN_ID NUMBER(38,0), NAME_TRACKER VARCHAR(128), V_TRACKER VARCHAR(100), V_COLLECTOR VARCHAR(100) NOT NULL, V_ETL VARCHAR(100) NOT NULL, USER_ID VARCHAR(255), USER_IPADDRESS VARCHAR(128), USER_FINGERPRINT VARCHAR(128), DOMAIN_USERID VARCHAR(128), DOMAIN_SESSIONIDX NUMBER(38,0), NETWORK_USERID VARCHAR(128), GEO_COUNTRY VARCHAR(2), GEO_REGION VARCHAR(3), GEO_CITY VARCHAR(75), GEO_ZIPCODE VARCHAR(15), GEO_LATITUDE FLOAT, GEO_LONGITUDE FLOAT, GEO_REGION_NAME VARCHAR(100), IP_ISP VARCHAR(100), IP_ORGANIZATION VARCHAR(128), IP_DOMAIN VARCHAR(128), IP_NETSPEED VARCHAR(100), PAGE_URL VARCHAR(4096), PAGE_TITLE VARCHAR(2000), PAGE_REFERRER VARCHAR(4096), PAGE_URLSCHEME VARCHAR(16), PAGE_URLHOST VARCHAR(255), PAGE_URLPORT NUMBER(38,0), PAGE_URLPATH VARCHAR(3000), PAGE_URLQUERY VARCHAR(6000), PAGE_URLFRAGMENT VARCHAR(3000), REFR_URLSCHEME VARCHAR(16), REFR_URLHOST VARCHAR(255), REFR_URLPORT NUMBER(38,0), REFR_URLPATH VARCHAR(6000), REFR_URLQUERY VARCHAR(6000), REFR_URLFRAGMENT VARCHAR(3000), REFR_MEDIUM VARCHAR(25), REFR_SOURCE VARCHAR(50), REFR_TERM VARCHAR(255), MKT_MEDIUM VARCHAR(255), MKT_SOURCE VARCHAR(255), MKT_TERM VARCHAR(255), MKT_CONTENT VARCHAR(500), MKT_CAMPAIGN VARCHAR(255), SE_CATEGORY VARCHAR(1000), SE_ACTION VARCHAR(1000), SE_LABEL VARCHAR(4096), SE_PROPERTY VARCHAR(1000), SE_VALUE FLOAT, TR_ORDERID VARCHAR(255), TR_AFFILIATION VARCHAR(255), TR_TOTAL NUMBER(18,2), TR_TAX NUMBER(18,2), TR_SHIPPING NUMBER(18,2), TR_CITY VARCHAR(255), TR_STATE VARCHAR(255), TR_COUNTRY VARCHAR(255), TI_ORDERID VARCHAR(255), TI_SKU VARCHAR(255), TI_NAME VARCHAR(255), TI_CATEGORY VARCHAR(255), TI_PRICE NUMBER(18,2), TI_QUANTITY NUMBER(38,0), PP_XOFFSET_MIN NUMBER(38,0), PP_XOFFSET_MAX NUMBER(38,0), PP_YOFFSET_MIN NUMBER(38,0), PP_YOFFSET_MAX NUMBER(38,0), USERAGENT VARCHAR(1000), BR_NAME VARCHAR(50), BR_FAMILY VARCHAR(50), BR_VERSION VARCHAR(50), BR_TYPE VARCHAR(50), BR_RENDERENGINE VARCHAR(50), BR_LANG VARCHAR(255), BR_FEATURES_PDF BOOLEAN, BR_FEATURES_FLASH BOOLEAN, BR_FEATURES_JAVA BOOLEAN, BR_FEATURES_DIRECTOR BOOLEAN, BR_FEATURES_QUICKTIME BOOLEAN, BR_FEATURES_REALPLAYER BOOLEAN, BR_FEATURES_WINDOWSMEDIA BOOLEAN, BR_FEATURES_GEARS BOOLEAN, BR_FEATURES_SILVERLIGHT BOOLEAN, BR_COOKIES BOOLEAN, BR_COLORDEPTH VARCHAR(12), BR_VIEWWIDTH NUMBER(38,0), BR_VIEWHEIGHT NUMBER(38,0), OS_NAME VARCHAR(50), OS_FAMILY VARCHAR(50), OS_MANUFACTURER VARCHAR(50), OS_TIMEZONE VARCHAR(255), DVCE_TYPE VARCHAR(50), DVCE_ISMOBILE BOOLEAN, DVCE_SCREENWIDTH NUMBER(38,0), DVCE_SCREENHEIGHT NUMBER(38,0), DOC_CHARSET VARCHAR(128), DOC_WIDTH NUMBER(38,0), DOC_HEIGHT NUMBER(38,0), TR_CURRENCY VARCHAR(3), TR_TOTAL_BASE NUMBER(18,2), TR_TAX_BASE NUMBER(18,2), TR_SHIPPING_BASE NUMBER(18,2), TI_CURRENCY VARCHAR(3), TI_PRICE_BASE NUMBER(18,2), BASE_CURRENCY VARCHAR(3), GEO_TIMEZONE VARCHAR(64), MKT_CLICKID VARCHAR(128), MKT_NETWORK VARCHAR(64), ETL_TAGS VARCHAR(500), DVCE_SENT_TSTAMP TIMESTAMP_NTZ(9), REFR_DOMAIN_USERID VARCHAR(128), REFR_DVCE_TSTAMP TIMESTAMP_NTZ(9), DOMAIN_SESSIONID VARCHAR(128), DERIVED_TSTAMP TIMESTAMP_NTZ(9), EVENT_VENDOR VARCHAR(1000), EVENT_NAME VARCHAR(1000), EVENT_FORMAT VARCHAR(128), EVENT_VERSION VARCHAR(128), EVENT_FINGERPRINT VARCHAR(128), TRUE_TSTAMP TIMESTAMP_NTZ(9), LOAD_TSTAMP TIMESTAMP_NTZ(9), CONTEXTS_COM_SNOWPLOWANALYTICS_SNOWPLOW_UA_PARSER_CONTEXT_1 VARCHAR, CONTEXTS_COM_SNOWPLOWANALYTICS_SNOWPLOW_WEB_PAGE_1 VARCHAR, CONTEXTS_COM_IAB_SNOWPLOW_SPIDERS_AND_ROBOTS_1 VARCHAR, CONTEXTS_NL_BASJES_YAUAA_CONTEXT_1 VARCHAR, constraint EVENT_ID_PK primary key (EVENT_ID) ); Step 3: Load the data 3.1 Log into your web interface and click on Databases tab.\n3.2 Locate the SAMPLE_EVENTS_BASE table that you just created and select it.\n3.3 Click the Load Data button to open the Load Data wizard.\n3.4 Select the relevant warehouse from the dropdown list. Click Next.\n3.5 Within the Source Files section select Load files from your computer option and click the Select Files button. If you have not saved the sample file provided as an attachment above please do so. Navigate to the SAMPLE_EVENTS.csv and click the Upload then the Next button.\n3.6 Create a new File Format with the plus (+) symbol beside the dropdown list, give it a name and change the following settings of the default csv file formats:\nHeader lines to skip= 1 Field optionally enclosed by= Double Quote 3.7 Click the Load button (no need to alter the Load Options). Loading should take place within a couple of minutes.\nFor more details please check out the official Snowflake documentation.\nStep 4: Create the ATOMIC.SAMPLE_EVENTS table The Snowplow pipeline creates context fields as arrays, however for the web data model to work, these need to be converted to varchars. Run the below DDL statement in your SQL editor:\nCREATE OR REPLACE TABLE TARGET_DB.ATOMIC.SAMPLE_EVENTS AS ( SELECT APP_ID, PLATFORM, ETL_TSTAMP, COLLECTOR_TSTAMP, DVCE_CREATED_TSTAMP, EVENT, EVENT_ID, TXN_ID, NAME_TRACKER, V_TRACKER, V_COLLECTOR, V_ETL, USER_ID, USER_IPADDRESS, USER_FINGERPRINT, DOMAIN_USERID, DOMAIN_SESSIONIDX, NETWORK_USERID, GEO_COUNTRY, GEO_REGION, GEO_CITY, GEO_ZIPCODE, GEO_LATITUDE, GEO_LONGITUDE, GEO_REGION_NAME, IP_ISP, IP_ORGANIZATION, IP_DOMAIN, IP_NETSPEED, PAGE_URL, PAGE_TITLE, PAGE_REFERRER, PAGE_URLSCHEME, PAGE_URLHOST, PAGE_URLPORT, PAGE_URLPATH, PAGE_URLQUERY, PAGE_URLFRAGMENT, REFR_URLSCHEME, REFR_URLHOST, REFR_URLPORT, REFR_URLPATH, REFR_URLQUERY, REFR_URLFRAGMENT, REFR_MEDIUM, REFR_SOURCE, REFR_TERM, MKT_MEDIUM, MKT_SOURCE, MKT_TERM, MKT_CONTENT, MKT_CAMPAIGN, SE_CATEGORY, SE_ACTION, SE_LABEL, SE_PROPERTY, SE_VALUE, TR_ORDERID, TR_AFFILIATION, TR_TOTAL, TR_TAX, TR_SHIPPING, TR_CITY, TR_STATE, TR_COUNTRY, TI_ORDERID, TI_SKU, TI_NAME, TI_CATEGORY, TI_PRICE, TI_QUANTITY, PP_XOFFSET_MIN, PP_XOFFSET_MAX, PP_YOFFSET_MIN, PP_YOFFSET_MAX, REPLACE(USERAGENT, \u0026#39;\\\u0026#34;\u0026#39;, \u0026#39;\u0026#39;) as USERAGENT, BR_NAME, BR_FAMILY, BR_VERSION, BR_TYPE, BR_RENDERENGINE, BR_LANG, BR_FEATURES_PDF, BR_FEATURES_FLASH, BR_FEATURES_JAVA, BR_FEATURES_DIRECTOR, BR_FEATURES_QUICKTIME, BR_FEATURES_REALPLAYER, BR_FEATURES_WINDOWSMEDIA, BR_FEATURES_GEARS, BR_FEATURES_SILVERLIGHT, BR_COOKIES, BR_COLORDEPTH, BR_VIEWWIDTH, BR_VIEWHEIGHT, OS_NAME, OS_FAMILY, OS_MANUFACTURER, OS_TIMEZONE, DVCE_TYPE, DVCE_ISMOBILE, DVCE_SCREENWIDTH, DVCE_SCREENHEIGHT, DOC_CHARSET, DOC_WIDTH, DOC_HEIGHT, TR_CURRENCY, TR_TOTAL_BASE, TR_TAX_BASE, TR_SHIPPING_BASE, TI_CURRENCY, TI_PRICE_BASE, BASE_CURRENCY, GEO_TIMEZONE, MKT_CLICKID, MKT_NETWORK, ETL_TAGS, DVCE_SENT_TSTAMP, REFR_DOMAIN_USERID, REFR_DVCE_TSTAMP, DOMAIN_SESSIONID, DERIVED_TSTAMP, EVENT_VENDOR, EVENT_NAME, EVENT_FORMAT, EVENT_VERSION, EVENT_FINGERPRINT, TRUE_TSTAMP, LOAD_TSTAMP, PARSE_JSON(REPLACE(REPLACE(CONTEXTS_COM_SNOWPLOWANALYTICS_SNOWPLOW_UA_PARSER_CONTEXT_1,\u0026#39;\\\u0026#34;\u0026#39;, \u0026#39;\u0026#39;),\u0026#39;\u0026#39;\u0026#39;\u0026#39;,\u0026#39;\\\u0026#34;\u0026#39;)) as CONTEXTS_COM_SNOWPLOWANALYTICS_SNOWPLOW_UA_PARSER_CONTEXT_1, PARSE_JSON(REPLACE(REPLACE(CONTEXTS_COM_SNOWPLOWANALYTICS_SNOWPLOW_WEB_PAGE_1,\u0026#39;\\\u0026#34;\u0026#39;, \u0026#39;\u0026#39;),\u0026#39;\u0026#39;\u0026#39;\u0026#39;,\u0026#39;\\\u0026#34;\u0026#39;)) as CONTEXTS_COM_SNOWPLOWANALYTICS_SNOWPLOW_WEB_PAGE_1, PARSE_JSON(REPLACE(REPLACE(CONTEXTS_COM_IAB_SNOWPLOW_SPIDERS_AND_ROBOTS_1,\u0026#39;\\\u0026#34;\u0026#39;, \u0026#39;\u0026#39;),\u0026#39;\u0026#39;\u0026#39;\u0026#39;,\u0026#39;\\\u0026#34;\u0026#39;)) as CONTEXTS_COM_IAB_SNOWPLOW_SPIDERS_AND_ROBOTS_1, PARSE_JSON(REPLACE(REPLACE(CONTEXTS_NL_BASJES_YAUAA_CONTEXT_1,\u0026#39;\\\u0026#34;\u0026#39;, \u0026#39;\u0026#39;),\u0026#39;\u0026#39;\u0026#39;\u0026#39;,\u0026#39;\\\u0026#34;\u0026#39;)) as CONTEXTS_NL_BASJES_YAUAA_CONTEXT_1 FROM ATOMIC.SAMPLE_EVENTS_BASE ) Step 5: Drop the SAMPLE_EVENTS_BASE table DROP TABLE TARGET_DB.ATOMIC.SAMPLE_EVENTS_BASE You will now have the ATOMIC.SAMPLE_EVENTS created and loaded with sample data.\n"
},
{
	"uri": "https://snowplow-incubator.github.io/advanced-analytics-web-accelerator/en/modeling/modeling_1/",
	"title": "Install Snowplow dbt Package",
	"tags": [],
	"description": "",
	"content": "Step 1: Add snowplow-web package Add the snowplow-web package to your packages.yml file. The latest version can be found here\npackages: - package: snowplow/snowplow_web version: 0.9.1 Step 2: Install the package Install the package by running:\ndbt deps "
},
{
	"uri": "https://snowplow-incubator.github.io/advanced-analytics-web-accelerator/en/next_steps/next_steps_1/",
	"title": "Model your pipeline data",
	"tags": [],
	"description": "",
	"content": "At this stage you should:\nHave tracking and enrichment set-up Have some data in the ATOMIC.EVENTS table Enabled IAB, UA parser and YAUAA enrichments Have a working dbt project with the web model configurations for the sample data Step 1: Complete refresh of your Snowplow web package (Optional) If you would like to use your current dbt environment that you set-up during modelling the sample data you might want to start from scratch.\nWhile you can drop and recompute the incremental tables within this package using the standard --full-refresh flag, all manifest tables are protected from being dropped in production. Without dropping the manifest during a full refresh, the selected derived incremental tables would be dropped but the processing of events would resume from where the package left off (as captured by the snowplow_web_incremental_manifest table) rather than your snowplow__start_date.\nIn order to drop all the manifest tables and start again set the snowplow__allow_refresh variable to true at run time:\ndbt run --select snowplow_web tag:snowplow_web_incremental --full-refresh --vars \u0026#39;snowplow__allow_refresh: true\u0026#39; # or using selector flag dbt run --selector snowplow_web --full-refresh --vars \u0026#39;snowplow__allow_refresh: true\u0026#39; Step 2: Modify variables Assuming that you followed the guide on how to run the data model on the sample data, here we will only highlight the differences in the set-up:\nRemove the snowplow__events variable. This time the base table will be the default atomic.events, therefore no need to overwrite it.\nChange the snowplow__start_date variable according to the data you have in your events table.\nOptional:\nsnowplow__backfill_limit_days: The maximum number of days of new data to be processed since the latest event processed. Set it to 1. We suggest changing snowplow__backfill_limit_days to 1 whilst working in your dev environment initially so that you can test how your incremental runs work. You will only have a few days of data available at this stage and if you leave it at the default 30 days, you will model all your data in one go.\nStep 3: Run the model Execute the following either through your CLI or from within dbt Cloud\ndbt run --selector snowplow_web Depending on the period of data available since the snowplow__start_date and the snowplow__backfill_limit_days variable you might not process all your data during your first run. Each time the model runs it should display the period it processes and the timestamp of the last event processed for each model within the package. This gets saved in the snowplow__incremental_manifest table so you can always check the data processing state (see below).\n"
},
{
	"uri": "https://snowplow-incubator.github.io/advanced-analytics-web-accelerator/en/tracking/tracking_1/",
	"title": "Setup your tracking",
	"tags": [],
	"description": "",
	"content": "There are a number of options to implement Snowplow tracking in your website or single page application.\nSelect the required pathway to implement tracking on your project.\nJavascript React Angular Step 1: Download sp.js Add the sp.js file to your project directory. The latest version can be found here.\nStep 2: Add JS snippet Add the below snippet to all of the pages you would like to track. Make sure to update the link to the sp.js file\nTypically this will be placed into the \u0026lt;head\u0026gt; element of your page or in a similar, suitable, location if using a Single Page Application framework.\n\u0026lt;script type=\u0026#34;text/javascript\u0026#34; async=1 \u0026gt; ;(function (p, l, o, w, i, n, g) { if (!p[i]) { p.GlobalSnowplowNamespace = p.GlobalSnowplowNamespace || []; p.GlobalSnowplowNamespace.push(i); p[i] = function () { (p[i].q = p[i].q || []).push(arguments) }; p[i].q = p[i].q || []; n = l.createElement(o); g = l.getElementsByTagName(o)[0]; n.async = 1; n.src = w; g.parentNode.insertBefore(n, g) } }(window, document, \u0026#34;script\u0026#34;, \u0026#34;{{Link to sp.js file}}\u0026#34;, \u0026#34;snowplow\u0026#34;)); \u0026lt;script\u0026gt; Step 3: Configure the Tracker Call newTracker in the \u0026lt;script\u0026gt; tag, with the following arguments. This creates an instance of a basic tracker without any additional context.\nTracker Name: 'sp' Collector Url: '{{Url for Collector}}' window.snowplow(\u0026#39;newTracker\u0026#39;, \u0026#39;sp\u0026#39;, \u0026#39;{{Url for Collector}}\u0026#39;) In addition to the basic tracker, add the below optional arguments to the tracker to make use of some of Snowplow\u0026rsquo;s more advanced features.\nOptional Settings (JSON):\nappId: Identify events that occur on different applications platform: Identify the platform the event occurred on, in this case web cookieSameSite: Lax Not sure why, or what explanation to give, but is recommended window.snowplow(\u0026#39;newTracker\u0026#39;, \u0026#39;sp\u0026#39;, \u0026#39;{{Url for Collector}}\u0026#39;, { appId: \u0026#39;appId\u0026#39;, platform: \u0026#39;web\u0026#39;, cookieSameSite: \u0026#39;Lax\u0026#39;, }); NOTE: react-router-dom is required to implement tracking in a react app\nStep 1: Install browser-tracker package Install the @snowplow/browser-tracker via npm by running:\nnpm install @snowplow/browser-tracker Step 2: Import the tracker package In your src folder, create a file called tracker.js.\nImport the browser tracker into tracker.js with the below snippet:\nimport React from \u0026#39;react\u0026#39;; import { newTracker, trackPageView, enableActivityTracking } from \u0026#34;@snowplow/browser-tracker\u0026#34;; Step 3: Configure the tracker Create the tracker in tracker.js with the with the following arguments. This creates an instance of a basic tracker without any additional context.\nTracker Name: 'sp' Collector Url: '{{Url for Collector}}' let tracker = newTracker(\u0026#39;sp\u0026#39;, \u0026#39;{{Url for Collector}}\u0026#39;) In addition to the basic tracker, add the below optional arguments to the tracker to make use of some of Snowplow\u0026rsquo;s more advanced features.\nOptional Settings (JSON):\nappId: Identify events that occur on different applications platform: Identify the platform the event occurred on, in this case web cookieSameSite: Lax Not sure why, or what explanation to give, but is recommended let tracker = newTracker(\u0026#39;sp\u0026#39;, \u0026#39;{{Url for Collector}}\u0026#39;, { appId: \u0026#39;appId\u0026#39;, platform: \u0026#39;web\u0026#39;, cookieSameSite: \u0026#39;Lax\u0026#39;, }); Step 1: Install browser-tracker package Install the @snowplow/browser-tracker via npm by running:\nnpm install @snowplow/browser-tracker Step 2: Generate Snowplow service Run ng generate service snowplow to create snowplow.service.ts and snowplow.service.spec.ts within src/app.\nImport the browser tracker to snowplow.service.ts by adding the snippet below.\nimport { newTracker, trackPageView, enableActivityTracking, BrowserTracker } from \u0026#34;@snowplow/browser-tracker\u0026#34;; Step 3: Configure the tracker Create the tracker with the with the following arguments. This creates an instance of a basic tracker without any additional context.\nTracker Name: 'sp' Collector Url: '{{Url for Collector}}' Add the below snippet to snowplow.service.ts\nexport class SnowplowService { tracker: BrowserTracker = newTracker(\u0026#39;sp\u0026#39;, \u0026#39;{{Url for Collector}}\u0026#39;) } In addition to the basic tracker, add the below optional arguments to the tracker to make use of some of Snowplow\u0026rsquo;s more advanced features.\nOptional Settings (JSON):\nappId: Identify events that occur on different applications platform: Identify the platform the event occurred on, in this case web cookieSameSite: Lax Not sure why, or what explanation to give, but is recommended export class SnowplowService { tracker: BrowserTracker = newTracker(\u0026#39;sp\u0026#39;, \u0026#39;{{Url for Collector}}\u0026#39;, { appId: \u0026#39;appId\u0026#39;, platform: \u0026#39;web\u0026#39;, cookieSameSite: \u0026#39;Lax\u0026#39;, }) } "
},
{
	"uri": "https://snowplow-incubator.github.io/advanced-analytics-web-accelerator/en/visualisation/visualisation_1/",
	"title": "Streamlit",
	"tags": [],
	"description": "",
	"content": "Streamlit uses Python to build shareable dashboards without the need for front-end development experience.\nDownload the streamlit-visualisation project template and copy the unzipped folder to your project directory to get started.\nAttachments streamlit-project-web.zip (30 KB) Step 1: Install requirements Run the command below to install the project requirements and run the virtual environment\npipenv install pipenv shell Step 2: Setup Database Connection Open secrets.toml and add your Snowflake account and database details. Ensure secrets.toml is in .gitignore to keep your information safe.\n# .streamlit/secrets.toml [snowflake] user = \u0026#34;xxx\u0026#34; password = \u0026#34;xxx\u0026#34; account = \u0026#34;xxx\u0026#34; database = \u0026#34;xxx\u0026#34; schema = \u0026#34;xxx\u0026#34; Step 3: Add your schema to queries Open pageviews.sql and sessions.sql. Add your schema name to the queries.\nSELECT * FROM SCHEMA_NAME.snowplow_web_page_views SELECT * FROM SCHEMA_NAME.snowplow_web_sessions Step 4: Run the Streamlit dashboard Run the command below to run the streamlit locally\nstreamlit run Dashboard.py "
},
{
	"uri": "https://snowplow-incubator.github.io/advanced-analytics-web-accelerator/en/upload/",
	"title": "Upload sample data",
	"tags": [],
	"description": "",
	"content": "Upload sample data flowchart LR id1(Upload)--\u003eid2(Model)--\u003eid3(Visualise)--\u003eid4(Track)--\u003eid5(Enrich)--\u003eid6(Next steps) style id1 fill:#f9f,stroke:#000,stroke-width:4px A sample events dataset for your Snowflake warehouse has been provided. This will allow you to be able to start data modeling and getting familiar with Snowplow event data, without the need to have a working pipeline. This chapter will guide you through this process.\n"
},
{
	"uri": "https://snowplow-incubator.github.io/advanced-analytics-web-accelerator/en/modeling/modeling_2/",
	"title": "Set-up and run dbt Package",
	"tags": [],
	"description": "",
	"content": "This step assumes you have data in the ATOMIC.SAMPLE_EVENTS table which will be used to demonstrate how to set-up and run the snowplow-web dbt package to model Snowplow web data.\nStep 1: Set-up Variables The snowplow_web dbt package comes with a list of variables specified with a default value that you may need to overwrite in your own dbt project\u0026rsquo;s dbt_project.yml file. For details you can have a look at the installed package\u0026rsquo;s default variables which can be found at [dbt_project_name]/dbt_packages/snowplow_web/dbt_project.yml.\nFor the sake of simplicity we have selected the variables that you will most likely need to overwrite, the rest can be changed at a later stage if and when it is needed.\nsnowplow__start_date: The date of the first tracked event. snowplow__enable_iab: Variable that by default is disabled but needed if the IAB enrichment is used. snowplow__enable_ua: Variable that by default is disabled but needed if the UA enrichment is used. snowplow__enable_yauaa: Variable that by default is disabled but needed if the YAUAA enrichment is used. snowplow__events: Variable to overwrite the events table in case it is named differently. It would have to be modified when using the sample_events table as a base. Add the following snippet to the dbt_project.yml:\nvars: snowplow_web: snowplow__start_date: \u0026#39;2022-08-19\u0026#39; snowplow__enable_iab: true snowplow__enable_ua: true snowplow__enable_yauaa: true snowplow__events: \u0026#39;atomic.sample_events\u0026#39; Step 2: Add the selectors.yml to your project The web package provides a suite of suggested selectors to help run and test the models.\nThese are defined in the selectors.yml file within the package, however to use these model selections you will need to copy this file into your own dbt project directory.\nThis is a top-level file and therefore should sit alongside your dbt_project.yml file.\nStep 3: Run the model Execute the following either through your CLI or from within dbt Cloud\ndbt run --selector snowplow_web This should take a couple of minutes to run.\n"
},
{
	"uri": "https://snowplow-incubator.github.io/advanced-analytics-web-accelerator/en/tracking/tracking_2/",
	"title": "Tracking Events",
	"tags": [],
	"description": "",
	"content": "The trackers create data on user actions at a specific point in time. For example:\nLoading a web page Clicking a link Submitting a form A number of tracking events are available out of the box. These include, but aren\u0026rsquo;t limited to:\nPage views Heartbeats (Page Pings) Link clicks HTML form actions Pageviews and Page Pings In this section, we will implement page views and page pings.\nJS React Angular Step 1: Enable Activity Tracking First we will enable activity tracking to collect page ping events. This will allow us to monitor engagement and record how a user digests content on the page over time.\nminimumVisitLength : The number of seconds from page load before the first page ping occurs heartbeatDelay: The number of seconds between page pings Add the snippet to your \u0026lt;script\u0026gt; tag below the tracker instance.\nsnowplow(\u0026#39;enableActivityTracking\u0026#39;, { minimumVisitLength: 5, heartbeatDelay: 10 }); Step 2: Track Page View To track a page view, simply call `trackPageView'.\nsnowplow(\u0026#39;trackPageView\u0026#39;) Note: trackPageView should go after the page ping event in step 1\nStep 1: Enable Activity Tracking First we will enable activity tracking to collect page ping events. This will allow us to monitor engagement and record how a user digests content on the page over time.\nminimumVisitLength : The number of seconds from page load before the first page ping occurs heartbeatDelay: The number of seconds between page pings Add the snippet to your tracker.js file below the tracker instance.\nenableActivityTracking({ minimumVisitLength: 5, heartbeatDelay: 10, }); Step 2: Enable Pageview Tracking To track page views, we will first define a function called useLocationChange(). This will take advantage of useEffect, the useLocation hook from react-router-dom and the trackPageView function from browser-tracker.\nuseLocation(): returns an object, location, describing the current page. useEffect: Exececutes a function whenever location changes. In this case trackPageView() trackPageView(): Sends a Snowplow page view event to the collector URL. Add the below snippet to tracker.js\nconst useLocationChange = () =\u0026gt; { const location = useLocation(); React.useEffect(() =\u0026gt; { trackPageView(); }, [location]); }; export { tracker, useLocationChange }; Step 3: Add Tracking to App Import useLocationChange to your App.js file.\nimport { useLocationChange } from \u0026#39;./tracker\u0026#39;; Add the useLocationChange() method to your App() function in App.js.\nfunction App() { useLocationChange(); ... } export default App; Step 1: Enable Activity Tracking First we will enable activity tracking to collect page ping events. This will allow us to monitor engagement and record how a user digests content on the page over time.\nminimumVisitLength : The number of seconds from page load before the first page ping occurs heartbeatDelay: The number of seconds between page pings Add the snippet to your snowplow.service.ts in the SnowplowService class below the tracker configuration.\nconstructor() { enableActivityTracking({ minimumVisitLength: 5, heartbeatDelay: 10, }); } Step 2: Track Page View To track a page view, we will create a trackPageView() function which will make use of the built in Snowplow method.\nAdd the snippet to your snowplow.service.ts file below the constructor.\npublic trackPageView(): void { trackPageView() } Step 3: Import Snowplow Service Add the below snippet to app.component.ts.\nimport { Router, NavigationEnd } from \u0026#39;@angular/router\u0026#39;; import { SnowplowService } from \u0026#39;./snowplow.service\u0026#39;; Add the constructor to the AppComponent class in app.component.ts. This contains the call to trackPageView().\nconstructor(router: Router, snowplow: SnowplowService) { router.events.subscribe((evt) =\u0026gt; { if (evt instanceof NavigationEnd) { snowplow.trackPageView(); } }); } Optional Tracking In addition to page pings and pageviews, you can enable link and form tracking. This won\u0026rsquo;t be used in the model in later steps but can be used in your own analysis.\nLink Tracking - Captures the link\u0026rsquo;s href by default as well as the id, class and target of the link. Form Tracking - Tracks an event when a user focuses, changes or submits a form. JS React Angular Step 1: Link Click Tracking To enable link click tracking, call the enableLinkClickTracking method.\nsnowplow(\u0026#39;enableLinkClickTracking\u0026#39;); You only need to call the method once to track all the links on a page.\nStep 2: HTML Form Tracking To enable form tracking, simply call the enableFormTracking method.\nsnowplow(\u0026#39;enableFormTracking\u0026#39;); Step 1: Install Plugins First install the plugins via npm.\nnpm install @snowplow/browser-plugin-link-click-tracking npm install @snowplow/browser-plugin-form-tracking Step 2: Import Plugins Next import the plugins to your tracker.js file.\nimport { LinkClickTrackingPlugin, enableLinkClickTracking } from \u0026#39;@snowplow/browser-plugin-link-click-tracking\u0026#39;; import { FormTrackingPlugin, enableFormTracking } from \u0026#39;@snowplow/browser-plugin-form-tracking\u0026#39;; Add the two plugins to your tracker instance.\nlet tracker = newTracker(\u0026#39;sp\u0026#39;, \u0026#39;{{Url for Collector}}\u0026#39;, { plugins: [LinkClickTrackingPlugin(), FormTrackingPlugin()], }); Step 3: Enable Tracking Add the enableLinkClickTracking() and enableFormTracking() methods to the useEffect hook in tracker.js.\nconst useLocationChange = () =\u0026gt; { const location = useLocation(); React.useEffect(() =\u0026gt; { enableLinkClickTracking() // Enable link click tracking here enableFormTracking() // Enable form tracking here trackPageView(); }, [location]); }; Step 1: Install Plugins First install the plugins via npm.\nnpm install @snowplow/browser-plugin-link-click-tracking npm install @snowplow/browser-plugin-form-tracking Step 2: Import Plugins Next import the plugins to your snowplow.service.ts file.\nimport { LinkClickTrackingPlugin, enableLinkClickTracking } from \u0026#39;@snowplow/browser-plugin-link-click-tracking\u0026#39;; import { FormTrackingPlugin, enableFormTracking } from \u0026#39;@snowplow/browser-plugin-form-tracking\u0026#39;; Add the two plugins to your tracker instance in snowplow.service.ts .\ntracker: BrowserTracker = newTracker(\u0026#39;sp\u0026#39;, \u0026#39;{{Url for Collector}}\u0026#39;, { ... plugins: [LinkClickTrackingPlugin(), FormTrackingPlugin()], ... }) Step 3: Enable Tracking Add the enableLinkClickTracking() and enableFormTracking() methods to the SnowplowService class in snowplow.service.ts.\npublic enableLinkClickTracking(): void { enableLinkClickTracking() } public enableLFormTracking(): void { enableFormTracking() } Step 4: Add Tracking to App Finally, add the enableLinkClickTracking() and enableFormTracking() methods to the constructor in the AppComponent class in app.component.ts as below.\nconstructor(router: Router, snowplow: SnowplowService) { router.events.subscribe((evt) =\u0026gt; { if (evt instanceof NavigationEnd) { snowplow.enableLinkClickTracking(); // Enable link click tracking here snowplow.enableFormTracking(); // Enable form tracking here snowplow.trackPageView(); } }); } "
},
{
	"uri": "https://snowplow-incubator.github.io/advanced-analytics-web-accelerator/en/next_steps/next_steps_2/",
	"title": "Visualise your pipeline data",
	"tags": [],
	"description": "",
	"content": "Assuming you have already set-up your Streamlit project all you have to do is change the schema within the queries used to generate the visualisations.\nOpen pageviews.sql and sessions.sql. Change your schema name within the queries.\nSELECT * FROM NEW_SCHEMA_NAME.snowplow_web_page_views SELECT * FROM NEW_SCHEMA_NAME.snowplow_web_sessions "
},
{
	"uri": "https://snowplow-incubator.github.io/advanced-analytics-web-accelerator/en/modeling/",
	"title": "Modeling",
	"tags": [],
	"description": "",
	"content": " Modeling your Data flowchart LR id1(Upload)--\u003eid2(Model)--\u003eid3(Visualise)--\u003eid4(Track)--\u003eid5(Enrich)--\u003eid6(Next steps) style id2 fill:#f9f,stroke:#000,stroke-width:4px The snowplow-web dbt package transforms and aggregates the raw web event data collected from the Snowplow JavaScript tracker into a set of derived tables: page views, sessions and users. Modeling the data makes it easier to digest and derive business value from the Snowplow data either through AI or BI.\nIn this chapter you will learn how to set-up an run the snowplow-web package to model the sample data.\n"
},
{
	"uri": "https://snowplow-incubator.github.io/advanced-analytics-web-accelerator/en/tracking/tracking_3/",
	"title": "Adding Context",
	"tags": [],
	"description": "",
	"content": "Whilst the tracking set-up provides event data on user actions at a specific point in time, context describes the setting in which an event takes place. To describe the context of an event, we need to define and capture individual entities. For example:\nThe user performing an action The web page the action occured on A product that has been interacted with Together, these entities make up the context of an event.\nSimilar to the predefined events, a number of entities are available to implement out of the box including:\nwebPage Entity - Adds the Pageview ID session Entity - Information about the user session performanceTiming Entity - Calculate page performance metrics geolocation Entity - Information on the users location JS React Angular Step 1: Enable predefined entities The webPage entity is enabled by default in the JavaScript tracker. This is required for the dbt web model used in later steps.\nEnable the context by including the below context options in your tracker creation.\nwindow.snowplow(\u0026#39;newTracker\u0026#39;, \u0026#39;sp\u0026#39;, \u0026#39;{{Url for Collector}}\u0026#39;, { ... contexts: { webPage: true, } ... }); Each event sent with this tracker will now contain the data from each of these entities.\nStep 1: Enable predefined entities The webPage entity is enabled by default in the JavaScript tracker. This is required for the dbt web model used in later steps.\nEnable the context by including the below context options in your tracker creation.\nlet tracker = newTracker(\u0026#39;sp\u0026#39;, \u0026#39;{{Url for Collector}}\u0026#39;, { ... contexts: { webPage: true } ... }); Each event sent with this tracker will now contain the data from each of these entities.\nStep 1: Enable predefined entities The webPage entity is enabled by default in the JavaScript tracker. This is required for the dbt web model used in later steps.\nEnable the context by including the below context options in your tracker creation.\nexport class SnowplowService { tracker: BrowserTracker = newTracker(\u0026#39;sp\u0026#39;, \u0026#39;{{Url for Collector}}\u0026#39;, { ... contexts: { webPage: true, session: false } ... } }); Each event sent with this tracker will now contain the data from each of these entities.\n"
},
{
	"uri": "https://snowplow-incubator.github.io/advanced-analytics-web-accelerator/en/next_steps/next_steps_3/",
	"title": "Custom models",
	"tags": [],
	"description": "",
	"content": "If you have got to this stage, congratulations! You are ready to take action and use your Snowplow generated data to help your business grow.\nAs a next step you might want to check out our detailed guide on how to create custom models to adjust the snowplow-web data model to your own needs if the out-of-the box solution does not fully fit your needs.\n"
},
{
	"uri": "https://snowplow-incubator.github.io/advanced-analytics-web-accelerator/en/modeling/modeling_3/",
	"title": "Explore Snowplow data",
	"tags": [],
	"description": "",
	"content": "Data should now be loaded into your warehouse. In this section we will take a closer look at the output to mitigate data issues and get familiar with the derived tables.\nStep 1: Check the output schemas Head to the SQL editor of your choice (e.g.: Snowflake Web UI) to check the model\u0026rsquo;s output. You should be able to see three new schemas created:\n[your_custom_schema]_scratch: drop and recompute models that aid the incremental run [your_custom_schema]_derived: main output models you can use in your downstream models and reporting [your_custom_schema]_manifest: tables that help the integrity and core incremental logic of the model Step 2: Run dbt test Run our recommended selector specified tests to identify potential issues with the data:\ndbt test --selector snowplow_web_lean_tests Step 3: Explore your data Take some time to familiarise yourself with the derived tables. You could run a few simple queries such as the ones listed below:\nFind out the number of page reads using derived.snowplow_web_page_views: SQL script WITH READS AS ( SELECT PAGE_TITLE, COUNT(*) FROM [YOUR_CUSTOM_SCHEMA]_DERIVED.SNOWPLOW_WEB_PAGE_VIEWS WHERE ENGAGED_TIME_IN_S \u0026gt; 60 AND VERTICAL_PIXELS_SCROLLED \u0026gt; 5000 GROUP BY 1 ORDER BY 2 DESC ) SELECT * FROM READS Calculate the bounce rate using derived.snowplow_sessions: SQL script WITH BOUNCE_RATE AS ( SELECT FIRST_PAGE_URLPATH, COUNT(DISTINCT DOMAIN_SESSIONID) AS SESSIONS, COUNT(DISTINCT CASE WHEN PAGE_VIEWS = 1 THEN DOMAIN_SESSIONID END) / COUNT(DISTINCT DOMAIN_SESSIONID) AS BOUNCE_RATE FROM DERIVED.SNOWPLOW_WEB_SESSIONS GROUP BY 1 ORDER BY SESSIONS DESC ) SELECT * FROM BOUNCE_RATE Find out details about the highest engaged user using derived.snowplow_users: SQL script WITH ENGAGEMENT AS ( SELECT * FROM DERIVED.SNOWPLOW_WEB_USERS ORDER BY ENGAGED_TIME_IN_S DESC LIMIT 1 ) SELECT * FROM ENGAGEMENT Check out the database section of the documentation site for a full breakdown of what the output should look like.\n"
},
{
	"uri": "https://snowplow-incubator.github.io/advanced-analytics-web-accelerator/en/tracking/tracking_4/",
	"title": "Testing",
	"tags": [],
	"description": "",
	"content": "The Snowplow Chrome Extension can be used to ensure the event was emitted correctly but the browser extension does not check that the event was processed correctly.\nStep 1: Installation Install the Snowplow Chrome Extension, you may need to restart your browser. Step 2: Check your data Open up devtools (F12) and navigate to the Snowplow extension. You should see a list of Pageview and Page Ping events start to form as you interact with your site.\nClick on an event to get a breakdown of the data being captured.\n"
},
{
	"uri": "https://snowplow-incubator.github.io/advanced-analytics-web-accelerator/en/visualisation/",
	"title": "Visualisation",
	"tags": [],
	"description": "",
	"content": " Visualisation flowchart LR id1(Upload)--\u003eid2(Model)--\u003eid3(Visualise)--\u003eid4(Track)--\u003eid5(Enrich)--\u003eid6(Test) style id3 fill:#f9f,stroke:#000,stroke-width:4px Use Streamlit to visualise your Snowplow data to make it easier to identify patterns and trends in your data.\n"
},
{
	"uri": "https://snowplow-incubator.github.io/advanced-analytics-web-accelerator/en/tracking/",
	"title": "Tracking",
	"tags": [],
	"description": "",
	"content": " Tracking flowchart LR id1(Upload)--\u003eid2(Model)--\u003eid3(Visualise)--\u003eid4(Track)--\u003eid5(Enrich)--\u003eid6(Next steps) style id4 fill:#f9f,stroke:#000,stroke-width:4px Getting started with sending events using the JavaScript tracker is very similar to other web analytics vendors like Google Analytics and Adobe Analytics.\nOnce set-up, you will have the ability to send behavioral data to your Snowplow pipeline.\n"
},
{
	"uri": "https://snowplow-incubator.github.io/advanced-analytics-web-accelerator/en/enrich/",
	"title": "Enrich",
	"tags": [],
	"description": "",
	"content": " Enrich flowchart LR id1(Upload)--\u003eid2(Model)--\u003eid3(Visualise)--\u003eid4(Track)--\u003eid5(Enrich)--\u003eid6(Next steps) style id5 fill:#f9f,stroke:#000,stroke-width:4px The enrichment process adds extra properties and values to your collected data.\n"
},
{
	"uri": "https://snowplow-incubator.github.io/advanced-analytics-web-accelerator/en/next_steps/",
	"title": "Next steps",
	"tags": [],
	"description": "",
	"content": " Next steps flowchart LR id1(Upload)--\u003eid2(Model)--\u003eid3(Visualise)--\u003eid4(Track)--\u003eid5(Enrich)--\u003eid6(Next steps) style id6 fill:#f9f,stroke:#000,stroke-width:4px Now that you have set-up tracking and enrichment on your pipeline and generated some test events it is time to make use of what you have learned so far by modelling and visualising your own pipeline data.\n"
},
{
	"uri": "https://snowplow-incubator.github.io/advanced-analytics-web-accelerator/en/",
	"title": "Advanced Analytics for Web",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://snowplow-incubator.github.io/advanced-analytics-web-accelerator/en/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://snowplow-incubator.github.io/advanced-analytics-web-accelerator/en/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]